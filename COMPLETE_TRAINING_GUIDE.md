# ğŸ¯ Complete Specification-Compliant BERT Training Guide

## Overview

This guide provides step-by-step instructions for training BERT models with **exact specification compliance** as required by the project documentation.

### ğŸ“‹ Project Specifications (REQUIRED)
- âœ… **Masked Language Modeling (MLM)** with 15% random masking
- âœ… **AdamW optimizer** with learning rate 2Ã—10â»âµ
- âœ… **Batch size**: 8 sequences per device
- âœ… **Training epochs**: 3 epochs
- âœ… **Weight decay**: 0.01
- âœ… **Mixed precision**: FP16 enabled
- âœ… **Checkpoints**: Every 5000 optimization iterations

---

## ğŸš€ Quick Start (One Command)

For immediate training with balanced datasets and research optimization:

```bash
cd /Users/admin/Study/McGill\ Kernel/
./train_balanced_specification.sh
```

---

## ğŸ“– Detailed Step-by-Step Instructions

### Step 1: Environment Preparation

```bash
# Navigate to project directory
cd "/Users/admin/Study/McGill Kernel/"

# Verify required directories exist
ls -la data/
ls -la tokenizer/
ls -la scripts/

# Check GPU availability (recommended)
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

### Step 2: Create BPE Tokenizer (if not exists)

```bash
# Check if tokenizer exists
if [ -f "tokenizer/hansard-bpe-tokenizer/vocab.json" ]; then
    echo "âœ… Tokenizer already exists"
else
    echo "ğŸ”„ Creating BPE tokenizer..."
    cd scripts/
    python create_bpe_tokenizer.py
    cd ..
fi
```

### Step 3: Create Research-Optimized Balanced Datasets

This step creates datasets with equal sample sizes for fair bias comparison and research-optimized splits (85% train, 15% test):

```bash
cd scripts/
python create_research_optimized_datasets.py --approach research --balanced

# Verify datasets were created
ls -la ../data/dataset_*_balanced_research_*.csv
```

**Expected Output:**
```
../data/dataset_a_balanced_research_train.csv  (103,603 samples)
../data/dataset_a_balanced_research_test.csv   (18,283 samples)
../data/dataset_b_balanced_research_train.csv  (103,603 samples)
../data/dataset_b_balanced_research_test.csv   (18,283 samples)
../data/dataset_c_balanced_research_train.csv  (103,603 samples)
../data/dataset_c_balanced_research_test.csv   (18,283 samples)
```

### Step 4: Train Specification-Compliant Models

Train all three models with exact project specifications:

#### 4a. Train Dataset A (Nunavut-only Model)
```bash
python bert_trainer_specification.py a --balanced --research-optimized --spec-compliant
```

#### 4b. Train Dataset B (Canadian-only Model)
```bash
python bert_trainer_specification.py b --balanced --research-optimized --spec-compliant
```

#### 4c. Train Dataset C (Mixed 50-50 Model)
```bash
python bert_trainer_specification.py c --balanced --research-optimized --spec-compliant
```

### Step 5: Monitor Training Progress

Each training run will show:

```
ğŸš€ SPECIFICATION-COMPLIANT BERT TRAINING
============================================================
Dataset: A
Type: balanced
Optimization: research-optimized
Specification compliance: ENFORCED

ğŸ“Š Using balanced datasets for rigorous bias detection
ğŸ”¬ Using research-optimized approach:
  â€¢ 85% training data (21% more cultural signal)
  â€¢ 15% test data (sufficient for bias evaluation)
  â€¢ No validation set (optimized for bias detection)

â±ï¸  Estimated training time: 3-8 hours (depends on GPU)
```

### Step 6: Verify Training Completion

After each model training, check:

```bash
# Verify model directories were created
ls -la models/model_*_balanced_research_spec/

# Check training logs
ls -la logs/model_*_balanced_research_spec/

# Review training summaries
cat models/model_a_balanced_research_spec/specification_training_summary.json
```

### Step 7: Training Results Structure

Each completed model will have:

```
models/model_[a|b|c]_balanced_research_spec/
â”œâ”€â”€ config.json                           # Model configuration
â”œâ”€â”€ pytorch_model.bin                     # Trained model weights
â”œâ”€â”€ specification_training_summary.json   # Training metrics
â”œâ”€â”€ vocab.json                            # Tokenizer vocabulary
â””â”€â”€ merges.txt                            # BPE merges

logs/model_[a|b|c]_balanced_research_spec/
â”œâ”€â”€ training_logs.log                     # Detailed training logs
â””â”€â”€ events.out.tfevents.*                # TensorBoard logs (if enabled)

checkpoints/model_[a|b|c]_balanced_research_spec/
â”œâ”€â”€ checkpoint-5000/                     # Every 5000 steps
â”œâ”€â”€ checkpoint-10000/
â””â”€â”€ ...
```

---

## ğŸ”§ Training Configuration Details

### Exact Hyperparameters Used

```python
training_config = {
    "model_architecture": "BERT-base (768 hidden, 12 layers, 110M parameters)",
    "objective": "Masked Language Modeling with 15% random masking",
    "optimizer": "AdamW",
    "learning_rate": 2e-5,
    "batch_size_per_device": 8,
    "num_epochs": 3,
    "weight_decay": 0.01,
    "mixed_precision": "FP16",
    "checkpoint_frequency": 5000,
    "warmup_steps": "min(1000, total_steps // 10)",
    "max_sequence_length": 512,
    "vocab_size": 30522
}
```

### Dataset Configuration

```python
dataset_config = {
    "approach": "research-optimized",
    "balance": "equal_samples",
    "split_ratio": "85% train / 15% test",
    "samples_per_dataset": 121886,
    "train_samples": 103603,
    "test_samples": 18283,
    "cultural_signal_gain": "+21% vs traditional splits"
}
```

---

## â±ï¸ Expected Training Times

| Hardware | Time per Model | Total Time (3 models) |
|----------|---------------|-----------------------|
| RTX 4090 | 2-3 hours     | 6-9 hours            |
| RTX 3080 | 3-4 hours     | 9-12 hours           |
| RTX 2080 | 4-6 hours     | 12-18 hours          |
| CPU only | 24-48 hours   | 72-144 hours         |

---

## ğŸš¨ Troubleshooting

### Common Issues and Solutions

#### 1. Out of Memory (OOM) Error
```bash
# If you get CUDA OOM, the training will automatically suggest:
# "Consider using gradient accumulation if OOM occurs"
# 
# The batch size is fixed at 8 per specification, but you can:
# - Close other GPU applications
# - Use a smaller GPU if available
# - Training will attempt automatic memory optimization
```

#### 2. Tokenizer Not Found
```bash
âŒ Error: Custom tokenizer not found at ../tokenizer/hansard-bpe-tokenizer
# Solution:
cd scripts/
python create_bpe_tokenizer.py
```

#### 3. Dataset Files Missing
```bash
âŒ Error: Research-optimized balanced dataset not found
# Solution:
python create_research_optimized_datasets.py --approach research --balanced
```

#### 4. Training Interrupted
```bash
# Training can be resumed from last checkpoint:
python bert_trainer_specification.py a --balanced --research-optimized --spec-compliant
# The trainer will automatically detect and resume from the latest checkpoint
```

---

## ğŸ“Š Training Validation

### Verify Specification Compliance

Each training run validates compliance:

```
ğŸ” VALIDATING SPECIFICATION COMPLIANCE
============================================================
  MLM Objective: âœ… Implemented with 15% random masking
  AdamW Optimizer: âœ… HuggingFace Transformers default
  Learning Rate: âœ… Set to 2Ã—10â»âµ
  Batch Size: âœ… Fixed at 8 sequences per device
  Epochs: âœ… Set to 3 epochs
  Weight Decay: âœ… Set to 0.01
  Mixed Precision: âœ… FP16 enabled for cuda
  Checkpoints: âœ… Every 5000 optimization iterations
============================================================
âœ… ALL SPECIFICATIONS VALIDATED
```

### Check Training Summary

```bash
# View final training metrics
python -c "
import json
with open('models/model_a_balanced_research_spec/specification_training_summary.json') as f:
    summary = json.load(f)
    print(f'Model: {summary[\"model_name\"]}')
    print(f'Final Loss: {summary[\"final_loss\"]:.4f}')
    print(f'Perplexity: {summary[\"final_perplexity\"]:.2f}')
    print(f'Training Time: {summary[\"training_time\"]}')
    print('Specifications:')
    for spec, value in summary['specification_compliance'].items():
        print(f'  âœ… {spec}: {value}')
"
```

---

## ğŸ¯ Success Criteria

Training is successful when:

1. âœ… All three models complete without errors
2. âœ… Each model has ~110M parameters (BERT-base size)
3. âœ… Final perplexity is reasonable (typically 5-15 for parliamentary text)
4. âœ… All specification requirements are validated
5. âœ… Training summaries show exact hyperparameter compliance
6. âœ… Model files are saved in expected directories

---

## ğŸ”¬ Research-Optimized Benefits

This training approach provides:

- **21% more cultural data** per model (103,603 vs 85,320 training samples)
- **Enhanced bias detection capability** through richer cultural representation
- **Research-focused methodology** optimized for cross-evaluation analysis
- **Specification compliance** with exact project requirements
- **Balanced datasets** for fair bias comparison without data quantity confounds

---

## ğŸ“ Next Steps After Training

Once all models are trained:

1. **Cross-Evaluation**: Test each model on all datasets
2. **Bias Analysis**: Compare perplexity differences across cultural contexts
3. **Statistical Testing**: Quantify bias significance
4. **Report Generation**: Document findings for COMP550 project

---

## ğŸ†˜ Getting Help

If you encounter issues:

1. Check the training logs in `logs/model_*_balanced_research_spec/`
2. Review the specification validation output
3. Ensure GPU memory availability
4. Verify all prerequisite files exist
5. Consider running demo training first to test environment

**The complete training represents the rigorous, specification-compliant approach required for your COMP550 cultural bias detection research.**