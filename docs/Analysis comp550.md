

# Future Research and Improvements in Culturally Fair NLP

## Future Research Directions

- **Broader Linguistic Inclusion:** Future work should examine biases across a wider range of Indigenous languages beyond just Inuktitut. Most NLP models heavily favor a few high-resource languages, leaving thousands of other languages underrepresented ([The State and Fate of Linguistic Diversity and Inclusion in the NLP World](https://aclanthology.org/2020.acl-main.560.pdf#:~:text=Language technologies contribute to promoting,in NLP conferences to understand)). By incorporating multiple Indigenous languages, researchers can identify language-specific bias patterns and ensure that improvements are not narrowly tailored to one language. This broader inclusion would also test whether knowledge transfer from dominant languages causes similar bias propagation in different cultural contexts.
- **Expanded and Validated Bias Benchmarks:** The Canadian Indigenous Representation and Bias Evaluation (CIRBE) benchmark introduced in the study should be expanded and rigorously validated by experts. Currently, CIRBE has not been reviewed by human domain experts ([Bencheikh_Ojo_Saad_comp550_final_project-4 (1).pdf](file://file-6qzxr5lvpe92qhmgt5segj%23:~:text=as cirbe is not yet,their deployment in our society/)), which limits confidence in its findings. A next iteration could involve Indigenous scholars and community leaders to verify that prompts truly capture Indigenous perspectives and concerns. Additionally, extending bias benchmarks to other cultural and political discourse datasets (e.g. legislative records from other countries or minority groups) would help evaluate whether bias transfer issues observed in this study generalize across contexts. Robust, peer-reviewed benchmarks will enable more reliable comparisons of models’ cultural biases.
- **Cross-Architecture Bias Comparison:** It is important to compare how bias transfer manifests in different language model architectures beyond RoBERTa. While this study focused on RoBERTa, other models like GPT-series (auto-regressive LLMs) or multilingual BERT could exhibit bias in distinct ways. Prior work has shown that various pre-trained models (e.g. BERT, GPT-2, etc.) all harbor social biases to some degree ([Bencheikh_Ojo_Saad_comp550_final_project-4 (1).pdf](file://xn--file-6qzxr5lvpe92qhmgt5segj%23:~:text=moin nadeem, anna bethke, and,for computational linguistics, pages 53565371-f517f/)). Future research can fine-tune multiple architectures on the same culturally imbalanced data to see if certain model types are more prone to propagating dominant-culture bias. This comparison would inform whether the bias issues are architecture-agnostic or if specific design choices (like masked language modeling vs. causal language modeling) exacerbate or reduce bias transfer.
- **Culturally-Aware Bias Mitigation Strategies:** Developing targeted bias mitigation techniques for underrepresented languages and cultures is a key direction. Standard debiasing methods (e.g. for gender or racial bias in English) may not directly apply to Indigenous contexts ([Bencheikh_Ojo_Saad_comp550_final_project-4 (1).pdf](file://file-6qzxr5lvpe92qhmgt5segj%23:~:text=tolga bolukbasi, kai,curran associates inc/)). New strategies could involve balancing representations during training (up-sampling minority culture data or down-sampling majority data) and injecting culturally specific knowledge. For example, incorporating community-authored texts or using knowledge bases about Indigenous history could help the model form more accurate representations. These mitigation approaches should be tailored to capture nuances of Indigenous discourse, as generic debiasing algorithms might overlook cultural subtleties.
- **Domain-Adaptive Fine-Tuning:** Exploring domain adaptation techniques offers a way to fine-tune models in a more balanced manner without amplifying dominant cultural biases. Instead of naive mixing of datasets, researchers could use adaptive training regimes that maintain performance on the minority domain. For instance, sequential or multi-stage fine-tuning can first focus on Indigenous data before gradually introducing majority-culture data, or vice versa, to prevent the model from simply overwhelming the minority signal. Such domain-adaptive training may help the model learn from underrepresented discourse **without** forgetting or overshadowing it with the dominant discourse. This direction aligns with efforts to make NLP models generalize to new domains while preserving fairness by not over-privileging the source domain’s traits.
- **Adversarial and Contrastive Learning Methods:** Another promising avenue is to use adversarial learning or contrastive techniques to counteract biased representations in the model’s embeddings. Recent research suggests adversarial networks can help remove or reduce biases in word embeddings and language models ([DataSpace: Adversarial Learning for Bias Mitigation in Machine Translation](https://dataspace.princeton.edu/handle/88435/dsp015712m9658#:~:text=biases regarding sensitive attributes%2C such,further development for use in)). For example, an adversarial objective could be added during training to discourage the model from encoding features that distinguish between Indigenous and non-Indigenous discourse except for meaningful content differences. Contrastive learning could be employed to make the model’s representations of semantically similar concepts from different cultures closer in the embedding space, thereby reducing cultural skew. By deliberately training the model to *unlearn* or *forget* spurious cultural biases, these methods could yield embeddings that treat Indigenous perspectives more equitably.
- **Assessing Societal Impact:** Beyond technical enhancements, future research should investigate the long-term societal implications of bias in NLP systems. It is crucial to understand how biased language models might influence real-world policy discussions or public opinion, especially in contexts involving Indigenous rights and representation. NLP biases are not just abstract errors—they can reinforce harmful stereotypes or exclude minority viewpoints in public discourse (). Studies could involve simulations or case studies where model outputs are introduced in decision-making settings to observe potential impacts. By examining these downstream effects, researchers and policymakers can better grasp the urgency of mitigating biases and establish guidelines for the responsible deployment of language technologies in multicultural societies.

## Limitations and Areas for Improvement

- **Lack of Expert Validation:** A notable weakness of the current study is that the CIRBE bias benchmark lacks validation by human experts. The prompts and “favorable” vs “unfavorable” answers were devised by the researchers without confirmed input from Indigenous community leaders or scholars. This raises questions about the reliability of the bias measurements – some prompts might not fully capture Indigenous perspectives, or the designation of what counts as a validating versus invalidating completion could be disputable. The authors acknowledge this limitation, as CIRBE in its first iteration has not been reviewed externally ([Bencheikh_Ojo_Saad_comp550_final_project-4 (1).pdf](file://file-6qzxr5lvpe92qhmgt5segj%23:~:text=as cirbe is not yet,their deployment in our society/)). Without expert validation, there is a risk that the benchmark could contain unintended biases or miss important nuances, potentially skewing the evaluation results.
- **Narrow Domain Scope:** The dataset used for training and evaluation is confined to Canadian parliamentary discourse (Hansard transcripts), which limits generalizability. Bias patterns learned in political debate transcripts may not reflect how biases appear in other domains like social media, news articles, or everyday conversations. Moreover, the cultural context is specific to Canadian and Inuit (Nunavut) discourse, so the findings might not transfer to Indigenous groups elsewhere or to different societal contexts. This narrow focus means the study’s conclusions are somewhat context-dependent – an NLP model could behave differently when encountering, say, Maori or Navajo text, or policy discussions outside of parliament. As a result, it’s unclear if the observed bias transfer effects are universal or a product of this particular domain and dataset composition.
- **Limited Evaluation Metrics:** The methodology for evaluating bias primarily relied on two techniques: cosine similarity of word embeddings and masked language modeling (predicting favorable vs unfavorable tokens in prompts). While these provide insight into semantic associations and model completions, they offer a limited view of bias. Cosine similarity captures one aspect of embedding geometry but might miss other manifestations of bias, and it can be sensitive to how the embedding space is structured ([Bencheikh_Ojo_Saad_comp550_final_project-4 (1).pdf](file://xn--file-6qzxr5lvpe92qhmgt5segj%23:~:text=maria antoniak and david mimno,association for computational linguistics, 6:107119-8c81g/)). The masked LM tests were formulated as binary completions, which may oversimplify the complex ways bias can surface in generated text. By not including other evaluation metrics – for example, measuring biases in downstream tasks (like classification outcomes), or using human judgment of model outputs – the analysis might overlook some biased behaviors. A more comprehensive suite of metrics could strengthen confidence that biases are being detected accurately from multiple angles.
- **Lack of Fine-Grained Bias Analysis:** The study does not delve into fine-grained or intersectional bias categories. It treats “Indigenous vs. non-Indigenous” as a broad axis of bias, but within the Indigenous content there could be additional biases (e.g., gender biases affecting Indigenous women, or biases about urban vs rural Indigenous communities). Intersectional bias – where multiple aspects like ethnicity, gender, and socioeconomic status intersect – is well-known to create unique forms of disadvantage that single-category analysis can miss (). By not examining these, the research may gloss over how an underrepresented group’s portrayal is not monolithic. For instance, an NLP model might treat Indigenous male voices differently from Indigenous female voices if both gender and cultural biases are in play. The absence of this fine-grained analysis is an area for improvement, as addressing intersectional biases is increasingly recognized as crucial for fairness in AI.
- **No Discussion of Ethical Considerations:** The paper’s discussion lacks a thorough examination of the ethical implications of deploying biased models. Given the findings – that underrepresentation of Inuit perspectives leads to measurably biased outputs – one would expect a conversation about the potential harm if such a model were used in real applications (e.g., in a government chatbot or an educational tool). There is little mention of responsible AI practices or how one might safeguard against the misuse of a model that could marginalize Indigenous voices. This is a shortcoming because acknowledging ethical risks and societal harm is an important part of NLP research, especially on sensitive topics (). Without this, readers and practitioners might not fully appreciate why bias mitigation matters. Incorporating an ethical perspective (such as discussing informed consent for using Indigenous data, or the impact on users who are Indigenous) would improve the study’s completeness and guide others in applying its results conscientiously.
- **Use of Translated Data Only:** Another limitation is that all the Indigenous discourse used for fine-tuning was in English translation rather than in the original Inuktitut language. The models were “unilingual and anglophone,” merely learning from translated Inuktitut content ([Bencheikh_Ojo_Saad_comp550_final_project-4 (1).pdf](file://file-6qzxr5lvpe92qhmgt5segj%23:~:text=ture datasets: the imbalanced multilingual,investigating bias transfer in linguistic/)). Relying solely on translations can introduce subtle biases or loss of meaning – nuances specific to Inuktitut might not survive translation, and any biases in the translation process itself could affect the training. Moreover, this approach doesn’t tell us how the model would handle actual Inuktitut text, since it never saw the Indigenous language directly. As a result, the findings pertain to how Indigenous *topics* are represented in an English context, rather than true multilingual ability or bias in the model. This is a constrained view of cultural bias transfer. Ideally, one would want to know if similar biases occur when the model processes text in the Indigenous language, or if the translation step either dilutes or exaggerates certain biases. By not training or evaluating on Inuktitut text, the study leaves that question open.

## Recommendations for Enhancement

- **Incorporate Expert Validation:** To address the benchmark validity issue, future work should involve Indigenous experts in curating and validating the CIRBE benchmark. Having community representatives review the prompts and expected “favorable” answers would ensure they authentically reflect Indigenous viewpoints. Expert feedback can help refine ambiguous cases and add important scenarios that researchers might have missed. This validation will not only improve the reliability of bias evaluations but also lend credibility and acceptance of the benchmark within affected communities. Ultimately, a benchmark co-developed with Indigenous stakeholders is more likely to measure culturally relevant biases and less likely to contain inadvertent prejudices.
- **Generalize to Other Domains:** Improving the generalizability of the findings is crucial. We recommend extending the analysis to diverse domains and corpora beyond Canadian parliamentary records. For example, researchers could apply similar techniques to media articles, social media posts, or literature involving Indigenous communities. Different genres might reveal new bias patterns – a model might perform acceptably on formal political text but behave prejudicially in casual social media language, or vice versa. By testing and fine-tuning models on a variety of datasets (while keeping cultural representation balanced in each), we can identify whether certain biases are domain-specific. This broader approach would strengthen the claim that a model is fair across contexts or highlight where additional mitigation is needed.
- **Train on Indigenous Languages Directly:** To overcome the limitations of using translated text, future experiments should train language models on Indigenous languages (like Inuktitut) directly, using multilingual or language-specific models. Modern multilingual models (such as mBERT or XLM-R) are capable of handling text in many languages, so fine-tuning them on the original Inuktitut corpus could show how biases manifest without the mediation of translation. Prior research warns that zero-shot or translation-based transfers have limitations in truly capturing low-resource languages ([Bencheikh_Ojo_Saad_comp550_final_project-4 (1).pdf](file://file-6qzxr5lvpe92qhmgt5segj%23:~:text=anne lauscher, vinit ravishankar, ivan,association for computational linguistics/)), which supports the idea of training natively. If resources allow, creating or using a pre-trained Indigenous language model, or at least a model that jointly learns from English and Indigenous texts, would likely give a fairer and more accurate representation. This direct approach could uncover biases that are unique to the Indigenous language or alleviate those introduced by translation artifacts.
- **Enhanced Fairness Evaluation Techniques:** It would be beneficial to adopt more sophisticated fairness and bias evaluation methods to complement cosine similarity and MLM probing. Causal inference techniques, for instance, could help determine if changing the cultural context of a sentence (while holding other factors constant) causes a significant change in model predictions – a sign of bias. Interpretability tools could be used to inspect which internal neurons or attention heads react to Indigenous vs. non-Indigenous cues, shedding light on how the model encodes culture. Furthermore, one could use controlled generation tests (prompting the model to continue text in different cultural styles) or employ human evaluators to judge outputs for bias. Using a combination of intrinsic measures (like embedding similarities), extrinsic task performance (e.g., does the model make more errors for Indigenous content), and human judgment would provide a more holistic assessment of bias. This multi-metric evaluation would make bias detection more robust and insights more actionable.
- **Stronger Bias Mitigation Strategies:** To actively reduce biases, researchers should implement and test more rigorous debiasing interventions. One strategy is **data balancing** – ensuring the training set has a proportionate representation of Indigenous and dominant-culture content (or even over-sampling the minority class) so the model cannot ignore the minority perspective. Another approach is **counterfactual data augmentation**, where for each piece of text, one could create a variant that swaps cultural references (Indigenous ↔ non-Indigenous) but keeps meaning similar ([Detecting and mitigating bias in natural language processing](https://www.brookings.edu/articles/detecting-and-mitigating-bias-in-natural-language-processing/#:~:text=Detecting and mitigating bias in,in AI and emerging technologies)) ([DataSpace: Adversarial Learning for Bias Mitigation in Machine Translation](https://dataspace.princeton.edu/handle/88435/dsp015712m9658#:~:text=biases regarding sensitive attributes%2C such,further development for use in)). Training the model on such pairs teaches it to treat those contexts more equally. There are also **fairness-aware training objectives** that penalize the model when its representations or outputs exhibit too much correlation with a sensitive attribute (like whether a sentence is from Nunavut or not), encouraging neutral encodings. These methods, used in prior work on gender or racial bias, should be adapted to the cultural context here. By experimenting with techniques like adversarial training (as mentioned, to remove telltale cultural signals that lead to biased outcomes) or contrastive learning, we can identify what combination of approaches most reduces harmful bias while preserving the model’s overall language understanding capabilities.
- **Longitudinal Bias Monitoring:** An important long-term improvement is to monitor how biases evolve as models are trained on progressively more balanced data. Instead of a one-off study, researchers could conduct a **longitudinal analysis**: for example, train successive model versions with increasing amounts of Indigenous content or with improved debiasing techniques, and track bias metrics over time. This would reveal if interventions are truly effective in the long run or if models find new ways to encode old biases. It also mirrors real-world deployment, where models might be updated regularly. Observing that a model’s bias on CIRBE prompts decreases as the data becomes more representative (or as new training methods are applied) would provide empirical evidence guiding best practices. Conversely, if bias plateaus or re-emerges, that indicates more innovative solutions are needed. Such continual evaluation aligns with the idea of “benchmarking progress” on fairness and can inform policy-makers and engineers about the state of NLP model fairness before those models are integrated into systems that impact people’s lives.

In summary, by addressing these weaknesses and pursuing new research directions, we can work toward NLP models that treat all cultures and languages with fairness. The study at hand has highlighted a critical problem – the transfer of bias from dominant to marginalized language contexts – and it opens the door for comprehensive solutions. Improving data diversity, evaluation rigor, and mitigation techniques will not only make models like RoBERTa more equitable for Indigenous Canadians, but also set a precedent for inclusive NLP practices worldwide.