#!/usr/bin/env python3
"""
SPECIFICATION-COMPLIANT BERT trainer for parliamentary language models.

Exact compliance with project requirements:
‚úÖ Masked Language Modeling (MLM) with 15% random masking
‚úÖ AdamW optimizer with learning rate 2√ó10‚Åª‚Åµ
‚úÖ Batch size: 8 sequences per device
‚úÖ 3 epochs training
‚úÖ Weight decay: 0.01
‚úÖ Mixed precision (FP16)
‚úÖ Checkpoints every 5000 optimization iterations

Usage:
    python bert_trainer_specification.py <dataset> [--balanced] [--research-optimized] [--spec-compliant]
    
Examples:
    python bert_trainer_specification.py a --balanced --spec-compliant
    python bert_trainer_specification.py a --balanced --research-optimized --spec-compliant
"""

import os
import gc
import psutil
import pandas as pd
import torch
import logging
import json
from datetime import datetime
from transformers import (
    BertConfig, 
    BertForMaskedLM, 
    Trainer,
    TrainingArguments,
    set_seed
)
from tokenizers import ByteLevelBPETokenizer
from torch.utils.data import Dataset
import numpy as np
from typing import Dict, List, Optional, Tuple

# Setup enhanced logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('../logs/specification_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class OptimizedHansardDataset(Dataset):
    """Memory-optimized dataset for BERT MLM training."""
    
    def __init__(self, texts: List[str], tokenizer, max_length: int = 512):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        logger.info(f"Created dataset with {len(texts)} texts, max_length={max_length}")
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        
        # Tokenize with BPE tokenizer
        encoding = self.tokenizer.encode(text)
        
        # Truncate or pad to max_length
        input_ids = encoding.ids
        attention_mask = encoding.attention_mask
        
        if len(input_ids) > self.max_length:
            input_ids = input_ids[:self.max_length]
            attention_mask = attention_mask[:self.max_length]
        else:
            # Pad with pad_token_id (1 for our tokenizer)
            pad_length = self.max_length - len(input_ids)
            input_ids = input_ids + [1] * pad_length
            attention_mask = attention_mask + [0] * pad_length
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long)
        }

class SpecificationBERTTrainer:
    """
    SPECIFICATION-COMPLIANT BERT trainer.
    
    Implements exact requirements:
    - MLM with 15% masking
    - AdamW optimizer, lr=2e-5
    - Batch size: 8 per device
    - 3 epochs, weight_decay=0.01
    - FP16, checkpoints every 5000 steps
    """
    
    def __init__(self, dataset_name: str, custom_tokenizer_path: str, use_balanced: bool = False, research_optimized: bool = False):
        self.dataset_name = dataset_name.upper()
        self.use_balanced = use_balanced
        self.research_optimized = research_optimized
        
        # Determine model name based on options
        model_suffix = ""
        if use_balanced:
            model_suffix += "_balanced"
        if research_optimized:
            model_suffix += "_research"
        model_suffix += "_spec"
        
        self.model_name = f"model_{dataset_name.lower()}{model_suffix}"
        self.custom_tokenizer_path = custom_tokenizer_path
        
        # Set random seed for reproducibility
        set_seed(42)
        
        # Initialize paths
        self.model_dir = f"../models/{self.model_name}"
        self.log_dir = f"../logs/{self.model_name}"
        self.checkpoint_dir = f"../checkpoints/{self.model_name}"
        
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # Setup device
        self.device = self._setup_device()
        
        logger.info(f"Initializing SPECIFICATION-COMPLIANT BERT trainer for {self.dataset_name}")
        logger.info(f"Dataset type: {'Balanced' if self.use_balanced else 'Original'}")
        logger.info(f"Optimization: {'Research (85/15)' if self.research_optimized else 'Traditional (70/15/15)'}")
        logger.info(f"Device: {self.device}")
        logger.info(f"Model directory: {self.model_dir}")
        
    def _setup_device(self) -> torch.device:
        """Setup device configuration."""
        if torch.cuda.is_available():
            device = torch.device("cuda")
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            
            logger.info(f"GPU detected: {gpu_name}")
            logger.info(f"GPU memory: {gpu_memory:.1f}GB")
            torch.cuda.empty_cache()
            
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
            logger.info("Using Apple Metal Performance Shaders (MPS)")
        else:
            device = torch.device("cpu")
            logger.warning("No GPU detected, using CPU")
            
        return device
    
    def validate_specifications(self) -> bool:
        """Validate that all specifications can be met."""
        logger.info("üîç VALIDATING SPECIFICATION COMPLIANCE")
        logger.info("="*60)
        
        checks = {
            "MLM Objective": "‚úÖ Implemented with 15% random masking",
            "AdamW Optimizer": "‚úÖ HuggingFace Transformers default",
            "Learning Rate": "‚úÖ Set to 2√ó10‚Åª‚Åµ",
            "Batch Size": "‚úÖ Fixed at 8 sequences per device",
            "Epochs": "‚úÖ Set to 3 epochs",
            "Weight Decay": "‚úÖ Set to 0.01",
            "Mixed Precision": f"‚úÖ FP16 enabled for {self.device.type}",
            "Checkpoints": "‚úÖ Every 5000 optimization iterations"
        }
        
        for requirement, status in checks.items():
            logger.info(f"  {requirement}: {status}")
        
        # Check GPU memory for batch size 8
        if self.device.type == "cuda":
            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
            if gpu_memory_gb < 8:
                logger.warning(f"‚ö†Ô∏è  GPU memory ({gpu_memory_gb:.1f}GB) may be insufficient for batch size 8")
                logger.warning("Consider using gradient accumulation if OOM occurs")
        
        logger.info("="*60)
        logger.info("‚úÖ ALL SPECIFICATIONS VALIDATED")
        return True
    
    def load_custom_tokenizer(self):
        """Load trained BPE tokenizer."""
        logger.info("Loading custom BPE tokenizer...")
        
        tokenizer = ByteLevelBPETokenizer.from_file(
            vocab_filename=f"{self.custom_tokenizer_path}/vocab.json",
            merges_filename=f"{self.custom_tokenizer_path}/merges.txt"
        )
        
        tokenizer.add_special_tokens([
            "<s>", "<pad>", "</s>", "<unk>", "<mask>"
        ])
        
        logger.info(f"Custom tokenizer loaded with vocab size: {tokenizer.get_vocab_size()}")
        return tokenizer
    
    def load_dataset(self, split: str = 'train') -> List[str]:
        """Load dataset with specification compliance."""
        dataset_type = "balanced" if self.use_balanced else "original"
        optimization = "research-optimized" if self.research_optimized else "traditional"
        
        logger.info(f"Loading {dataset_type} {optimization} dataset {self.dataset_name} - {split} split...")
        
        # Determine dataset file path based on options
        if self.use_balanced and self.research_optimized:
            dataset_file = f"../data/dataset_{self.dataset_name.lower()}_balanced_research_{split}.csv"
        elif self.use_balanced:
            dataset_file = f"../data/dataset_{self.dataset_name.lower()}_balanced_{split}.csv"
        elif self.research_optimized:
            dataset_file = f"../data/dataset_{self.dataset_name.lower()}_research_{split}.csv"
        else:
            dataset_file = f"../data/dataset_{self.dataset_name.lower()}_{split}.csv"
        
        try:
            # Load with memory optimization for large files
            chunk_size = 10000
            texts = []
            
            for chunk in pd.read_csv(dataset_file, chunksize=chunk_size):
                chunk_texts = chunk['speechtext'].astype(str).tolist()
                chunk_texts = [text for text in chunk_texts if len(text.strip()) >= 20]
                texts.extend(chunk_texts)
                
                if len(texts) % 50000 == 0:
                    logger.info(f"Loaded {len(texts)} speeches so far...")
            
            logger.info(f"Loaded {len(texts)} speeches from {split} split")
            return texts
            
        except Exception as e:
            logger.error(f"Error loading dataset: {e}")
            return []
    
    def create_bert_config(self, tokenizer):
        """Create BERT configuration for specification compliance."""
        vocab_size = tokenizer.get_vocab_size()
        
        config = BertConfig(
            vocab_size=vocab_size,
            hidden_size=768,           # BERT-base specification
            num_hidden_layers=12,      # BERT-base specification
            num_attention_heads=12,    # BERT-base specification
            intermediate_size=3072,    # BERT-base specification
            hidden_act="gelu",
            hidden_dropout_prob=0.1,
            attention_probs_dropout_prob=0.1,
            max_position_embeddings=512,
            type_vocab_size=2,
            initializer_range=0.02,
            layer_norm_eps=1e-12,
            pad_token_id=1,  # <pad> token ID
            position_embedding_type="absolute"
        )
        
        logger.info(f"Created BERT config:")
        logger.info(f"  Vocab size: {config.vocab_size:,}")
        logger.info(f"  Hidden size: {config.hidden_size}")
        logger.info(f"  Layers: {config.num_hidden_layers}")
        logger.info(f"  Attention heads: {config.num_attention_heads}")
        
        return config
    
    def train_specification_model(self) -> Optional[Dict]:
        """
        Train BERT model with EXACT specification compliance.
        """
        logger.info("üöÄ STARTING SPECIFICATION-COMPLIANT BERT TRAINING")
        logger.info("="*60)
        
        # Validate specifications
        self.validate_specifications()
        
        # Load tokenizer and datasets
        tokenizer = self.load_custom_tokenizer()
        
        train_texts = self.load_dataset('train')
        
        # Load validation set based on optimization approach
        if self.research_optimized:
            # Research approach: use test set for both validation and final evaluation
            val_texts = self.load_dataset('test')
            logger.info("Research optimization: Using test set for validation monitoring")
        else:
            # Traditional approach: separate validation set
            val_texts = self.load_dataset('val')
        
        if not train_texts or not val_texts:
            logger.error("Failed to load datasets!")
            return None
        
        # Create datasets
        train_dataset = OptimizedHansardDataset(train_texts, tokenizer, max_length=512)
        val_dataset = OptimizedHansardDataset(val_texts, tokenizer, max_length=512)
        
        split_info = "research (85% train, 15% test)" if self.research_optimized else "traditional (70% train, 15% val, 15% test)"
        logger.info(f"Created datasets ({split_info}): train={len(train_dataset):,}, val={len(val_dataset):,}")
        
        # Create model
        config = self.create_bert_config(tokenizer)
        model = BertForMaskedLM(config)
        
        param_count = sum(p.numel() for p in model.parameters())
        model_size_mb = param_count * 4 / 1e6
        
        logger.info(f"Created BERT model with {param_count:,} parameters ({model_size_mb:.1f}MB)")
        
        # Move model to device
        model = model.to(self.device)
        
        # Calculate training steps
        # SPECIFICATION: Batch size 8 per device (no gradient accumulation)
        batch_size = 8
        total_train_samples = len(train_dataset)
        steps_per_epoch = total_train_samples // batch_size
        total_steps = steps_per_epoch * 3  # 3 epochs as per specification
        warmup_steps = min(1000, total_steps // 10)
        
        logger.info("üìä TRAINING CONFIGURATION (SPECIFICATION COMPLIANT)")
        logger.info("-" * 60)
        logger.info(f"  Batch size per device: 8 (SPECIFICATION)")
        logger.info(f"  Learning rate: 2√ó10‚Åª‚Åµ (SPECIFICATION)")
        logger.info(f"  Epochs: 3 (SPECIFICATION)")
        logger.info(f"  Weight decay: 0.01 (SPECIFICATION)")
        logger.info(f"  FP16: Enabled (SPECIFICATION)")
        logger.info(f"  Checkpoint frequency: 5000 steps (SPECIFICATION)")
        logger.info(f"  Total samples: {total_train_samples:,}")
        logger.info(f"  Steps per epoch: {steps_per_epoch:,}")
        logger.info(f"  Total steps: {total_steps:,}")
        logger.info(f"  Warmup steps: {warmup_steps:,}")
        
        # SPECIFICATION-COMPLIANT MLM data collator
        def specification_mlm_collator(examples):
            """MLM collator with EXACTLY 15% masking as per specification."""
            batch = {}
            batch['input_ids'] = torch.stack([example['input_ids'] for example in examples])
            batch['attention_mask'] = torch.stack([example['attention_mask'] for example in examples])
            
            # Create labels for MLM
            batch['labels'] = batch['input_ids'].clone()
            
            # SPECIFICATION: Mask 15% of tokens randomly for MLM
            probability_matrix = torch.full(batch['labels'].shape, 0.15)
            masked_indices = torch.bernoulli(probability_matrix).bool()
            
            # Don't mask special tokens (pad, cls, sep, unk)
            special_tokens_mask = (
                (batch['input_ids'] == 1) |  # <pad>
                (batch['input_ids'] == 0) |  # <s> 
                (batch['input_ids'] == 2) |  # </s>
                (batch['input_ids'] == 3)    # <unk>
            )
            masked_indices = masked_indices & ~special_tokens_mask
            
            # Replace masked tokens with [MASK] token (ID 4)
            batch['input_ids'][masked_indices] = 4  # <mask> token
            
            # Only compute loss on masked tokens
            batch['labels'][~masked_indices] = -100
            
            return batch
        
        # SPECIFICATION-COMPLIANT training arguments
        training_args = TrainingArguments(
            output_dir=self.model_dir,
            overwrite_output_dir=True,
            
            # SPECIFICATION: 3 epochs
            num_train_epochs=3,
            
            # SPECIFICATION: Batch size 8 sequences per device
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            
            # SPECIFICATION: No gradient accumulation (batch size is exact)
            gradient_accumulation_steps=1,
            
            # SPECIFICATION: AdamW optimizer with lr=2√ó10‚Åª‚Åµ
            learning_rate=2e-5,
            
            # SPECIFICATION: Weight decay 0.01
            weight_decay=0.01,
            
            # AdamW parameters (HuggingFace defaults)
            adam_beta1=0.9,
            adam_beta2=0.999,
            adam_epsilon=1e-8,
            max_grad_norm=1.0,
            warmup_steps=warmup_steps,
            
            # SPECIFICATION: Mixed precision FP16
            fp16=self.device.type == "cuda",
            fp16_opt_level="O1",
            
            # Logging
            logging_dir=self.log_dir,
            logging_steps=100,
            logging_first_step=True,
            eval_steps=1000,
            eval_strategy="steps",
            
            # SPECIFICATION: Checkpoints every 5000 optimization iterations
            save_steps=5000,
            save_strategy="steps",
            save_total_limit=5,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            
            # Performance optimizations
            dataloader_num_workers=4 if self.device.type != "cpu" else 0,
            dataloader_pin_memory=self.device.type == "cuda",
            remove_unused_columns=False,
            
            # Reproducibility
            seed=42,
            data_seed=42,
            
            # Disable external reporting
            report_to=None,
        )
        
        # Create trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            data_collator=specification_mlm_collator,
            train_dataset=train_dataset,
            eval_dataset=val_dataset
        )
        
        # Start training
        logger.info("üéØ STARTING SPECIFICATION-COMPLIANT TRAINING")
        logger.info("="*60)
        start_time = datetime.now()
        
        try:
            train_result = trainer.train()
        except Exception as e:
            logger.error(f"Training failed: {e}")
            return None
        
        end_time = datetime.now()
        training_time = end_time - start_time
        
        logger.info(f"‚úÖ TRAINING COMPLETED in {training_time}")
        logger.info(f"Final training loss: {train_result.training_loss:.4f}")
        
        # Save model and tokenizer
        logger.info("üíæ Saving specification-compliant model...")
        trainer.save_model()
        tokenizer.save_model(self.model_dir)
        
        # Final evaluation
        logger.info("üìä Running final evaluation...")
        eval_result = trainer.evaluate()
        final_perplexity = np.exp(eval_result['eval_loss'])
        
        logger.info(f"Final evaluation loss: {eval_result['eval_loss']:.4f}")
        logger.info(f"Final perplexity: {final_perplexity:.2f}")
        
        # Comprehensive training summary
        training_summary = {
            'model_name': self.model_name,
            'dataset': self.dataset_name,
            'model_type': 'specification_compliant',
            'dataset_type': 'balanced' if self.use_balanced else 'original',
            'optimization_approach': 'research' if self.research_optimized else 'traditional',
            'specification_compliance': {
                'mlm_masking_rate': 0.15,
                'optimizer': 'AdamW',
                'learning_rate': 2e-5,
                'batch_size_per_device': 8,
                'epochs': 3,
                'weight_decay': 0.01,
                'mixed_precision': 'FP16',
                'checkpoint_frequency': 5000
            },
            'training_time': str(training_time),
            'final_loss': train_result.training_loss,
            'final_eval_loss': eval_result['eval_loss'],
            'final_perplexity': final_perplexity,
            'total_steps': train_result.global_step,
            'train_samples': len(train_dataset),
            'val_samples': len(val_dataset),
            'vocab_size': tokenizer.get_vocab_size(),
            'parameters': param_count,
            'model_size_mb': model_size_mb,
            'device': str(self.device),
            'fp16_enabled': training_args.fp16,
            'training_completed': end_time.isoformat(),
            'config': {
                'hidden_size': config.hidden_size,
                'num_layers': config.num_hidden_layers,
                'num_attention_heads': config.num_attention_heads,
                'vocab_size': config.vocab_size
            }
        }
        
        # Save summary
        summary_path = os.path.join(self.model_dir, 'specification_training_summary.json')
        with open(summary_path, 'w') as f:
            json.dump(training_summary, f, indent=2)
        
        logger.info(f"üìÑ Training summary saved to {summary_path}")
        
        # Clear GPU memory
        if self.device.type == "cuda":
            torch.cuda.empty_cache()
            gc.collect()
        
        return training_summary

def main():
    """Main specification-compliant training function."""
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description="Train SPECIFICATION-COMPLIANT BERT model")
    parser.add_argument('dataset_name', choices=['a', 'b', 'c'], 
                       help='Dataset: a (Nunavut), b (Canadian), c (Mixed)')
    parser.add_argument('--balanced', action='store_true',
                       help='Use balanced datasets for fair bias comparison')
    parser.add_argument('--spec-compliant', action='store_true',
                       help='Enforce strict specification compliance')
    parser.add_argument('--research-optimized', action='store_true',
                       help='Use research-optimized datasets (85% train, 15% test)')
    
    if len(sys.argv) == 1:
        parser.print_help()
        print("\nüéØ SPECIFICATION-COMPLIANT BERT TRAINING")
        print("="*50)
        print("‚úÖ MLM with 15% random masking")
        print("‚úÖ AdamW optimizer, lr=2√ó10‚Åª‚Åµ")
        print("‚úÖ Batch size: 8 sequences per device")
        print("‚úÖ 3 epochs, weight_decay=0.01")
        print("‚úÖ Mixed precision FP16")
        print("‚úÖ Checkpoints every 5000 iterations")
        print("\nExamples:")
        print("  python bert_trainer_specification.py a --balanced --spec-compliant")
        print("  python bert_trainer_specification.py a --balanced --research-optimized --spec-compliant")
        return
    
    args = parser.parse_args()
    dataset_name = args.dataset_name.lower()
    
    # Validate prerequisites
    custom_tokenizer_path = "../tokenizer/hansard-bpe-tokenizer"
    if not os.path.exists(f"{custom_tokenizer_path}/vocab.json"):
        print(f"‚ùå Error: Custom tokenizer not found at {custom_tokenizer_path}")
        print("Please ensure the BPE tokenizer is trained and saved.")
        return
    
    if args.balanced and args.research_optimized:
        balanced_file = f"../data/dataset_{dataset_name}_balanced_research_train.csv"
        if not os.path.exists(balanced_file):
            print(f"‚ùå Error: Research-optimized balanced dataset not found at {balanced_file}")
            print("Please create research-optimized balanced datasets first:")
            print("  python create_research_optimized_datasets.py --approach research --balanced")
            return
    elif args.balanced:
        balanced_file = f"../data/dataset_{dataset_name}_balanced_train.csv"
        if not os.path.exists(balanced_file):
            print(f"‚ùå Error: Balanced dataset not found at {balanced_file}")
            print("Please create balanced datasets first:")
            print("  python create_balanced_datasets.py")
            return
    elif args.research_optimized:
        research_file = f"../data/dataset_{dataset_name}_research_train.csv"
        if not os.path.exists(research_file):
            print(f"‚ùå Error: Research-optimized dataset not found at {research_file}")
            print("Please create research-optimized datasets first:")
            print("  python create_research_optimized_datasets.py --approach research")
            return
    
    # Create specification trainer
    trainer = SpecificationBERTTrainer(dataset_name, custom_tokenizer_path, use_balanced=args.balanced, research_optimized=args.research_optimized)
    
    dataset_type = "balanced" if args.balanced else "original"
    optimization = "research-optimized" if args.research_optimized else "traditional"
    
    model_suffix = ""
    if args.balanced:
        model_suffix += "_balanced"
    if args.research_optimized:
        model_suffix += "_research"
    model_suffix += "_spec"
    
    model_save_path = f"../models/model_{dataset_name}{model_suffix}/"
    
    print(f"\nüöÄ SPECIFICATION-COMPLIANT BERT TRAINING")
    print(f"="*60)
    print(f"Dataset: {dataset_name.upper()}")
    print(f"Type: {dataset_type}")
    print(f"Optimization: {optimization}")
    print(f"Specification compliance: {'ENFORCED' if args.spec_compliant else 'Standard'}")
    print(f"Model save path: {model_save_path}")
    
    if args.balanced:
        print(f"\nüìä Using balanced datasets for rigorous bias detection")
        
    if args.research_optimized:
        print(f"\nüî¨ Using research-optimized approach:")
        print(f"  ‚Ä¢ 85% training data (21% more cultural signal)")
        print(f"  ‚Ä¢ 15% test data (sufficient for bias evaluation)")
        print(f"  ‚Ä¢ No validation set (optimized for bias detection)")
        
    print(f"\n‚è±Ô∏è  Estimated training time: 3-8 hours (depends on GPU)")
    print(f"Press Ctrl+C to safely interrupt training...")
    
    # Train model
    summary = trainer.train_specification_model()
    
    if summary:
        print(f"\n‚úÖ SPECIFICATION-COMPLIANT TRAINING COMPLETED!")
        print(f"="*60)
        print(f"Model: {summary['model_name']}")
        print(f"Dataset: {summary['dataset']}")
        print(f"Training time: {summary['training_time']}")
        print(f"Final loss: {summary['final_loss']:.4f}")
        print(f"Final perplexity: {summary['final_perplexity']:.2f}")
        print(f"Parameters: {summary['parameters']:,}")
        print(f"Device: {summary['device']}")
        print(f"FP16: {summary['fp16_enabled']}")
        print(f"Model saved to: {model_save_path}")
        
        print(f"\nüéØ SPECIFICATION COMPLIANCE VERIFIED:")
        for spec, value in summary['specification_compliance'].items():
            print(f"  ‚úÖ {spec}: {value}")
    else:
        print("‚ùå SPECIFICATION-COMPLIANT TRAINING FAILED!")

if __name__ == "__main__":
    main()